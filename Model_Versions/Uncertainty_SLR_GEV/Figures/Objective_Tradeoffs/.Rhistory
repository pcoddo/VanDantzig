# Pennsylvania State University
# poddo@psu.edu
#
# Adapted from cost-benefit-slr.R
# Authored by Ryan Sriver and Klaus Keller
# Pennsylvania State University
#
# Based on: van Dantzig D (1956) Economic Decision Problems for flood prevention.
# Econometrica 24:276-287
###################################
# van Dantzig (1956) baseline model
# Includes only storm-surge uncertainty
###################################
rm(list = ls())
# Define parameters as given in van Dantzig
#	- check with Table 4.1 in ref 2
# flood frequency in 1/a at zero height increase
p_zero_p=0.0038
# rate of exponential flood frequency decrease per m height increase
alpha_p=2.6
# costs of flooding in Gulders (include the "factor of two" from the ms
V_p=2e10
# effective discount rate in % per year, this is net of increase in value damages
delta_prime_p=0.02
# costs of heightening per m in gulders.
k_p=4.2e7
# ime horizon to the next revision of the dike (in years), from paper
T=75
# subsidence rate in m/a, from the paper. Equivalent to 0.7 m per century.
subs_rate=0.007
# Example sea_level rise rate in m/a. Equivalent to 0.9 m per century
sea_level_rate=0.009
# time scale
beta_p=alpha_p*subs_rate
# considerd range of Deike heigtening in meters
X=seq(0,10,by=0.05)
# considered time horison, in annual increments
time=seq(0,T,by=1)
# Define variables
p_exceed=array(NA,dim=c(length(X)))
costs=array(NA,dim=c(length(X)))
NPV_expected_losses=array(NA,dim=c(length(X)))
EV_p_exceed_transient=array(NA,dim=c(length(X)))
Total_flood_frequency=array(NA,dim=c(length(X)))
total_costs=array(NA,dim=c(length(X)))
discount_factor=array(NA, dim=c(length(time)))
effective_height=array(NA, dim=c(length(X),length(time)))
p_exceed_transient=array(NA, dim=c(length(X),length(time)))
NPV_costs_flooding=array(NA, dim=c(length(X),length(time)))
subsidence=subs_rate*time
sea_level_rise=sea_level_rate*time
for(i in 1:length(X)) {
# analyze for each considered Deike heightening (eq. 1 in paper)
p_exceed[i]= p_zero_p*exp(-alpha_p*X[i])
for (j in 1:length(time)) {
# analze for each year of the initial time period
year =j-1
#losses in the future are discounted at the annual net discount rate
discount_factor[j]=1/(1+delta_prime_p)^year
#The effective deike height decreases with subsidence and sea-level rise.
effective_height[i,j]=X[i]-subsidence[j]-sea_level_rise[j]
# For a stationary flooding frequency, we can evaluate the annual flooding
#frequency with the old observeations and the new effective height.
p_exceed_transient[i,j]= p_zero_p*exp(-alpha_p*effective_height[i,j])
#The net present value of the losses per year are the product of the
#frequency of flooding per year, the damages per flood, and the discount factor.
NPV_costs_flooding[i,j]=p_exceed_transient[i,j]*V_p*discount_factor[j]
}
#The costs of building the dike increase linearly with respect to height.
costs[i]=k_p*X[i]
#The total discounted expected losses are the sum of the discounted expected annual losses.
NPV_expected_losses[i]=sum(NPV_costs_flooding[i,])
#The average annual flood frequency is the mean the annual flood frequencies.
EV_p_exceed_transient[i]=mean(p_exceed_transient[i,])
#The total flood frequency over the life-time of the project is the sum of the flood frequencies,
#assuminG independence, as in the original paper
Total_flood_frequency[i]=sum(p_exceed_transient[i,])
#The total costs that depend on the deike height. Note that the fixed
#costs of setting up the deike heightening as well as the effects of
#deike height on costs beyond the time horizon are neglected
total_costs[i]=costs[i]+NPV_expected_losses[i]
}
print(X)
print(p_exceed)
print(total_costs)
# find the minimum cost
min_ind=seq(along=total_costs)[total_costs == min(total_costs)]
min_cost_X=X[min_ind]
print(min_cost_X)
# find de-minimis risk defense
de_minimis_risk=1e-6  # define minimum risk height
# find height increase corresponding to min cost de-mininis risk defense
cost=total_costs[1]
for(i in 1:length(X)) {
if (Total_flood_frequency[i] < de_minimis_risk  && total_costs[i] < cost) {
min_cost_de_minimis_X=X[i]
cost=total_costs[i]
}
}
print(min_cost_de_minimis_X)
subs_rate * (2050-2015) * 1000
X <- matrix(rnorm(2000), ncol = 2)
X.chull <- chull (X)
X.chull <- c(X.chull, X.chull[1])
plot (X)
lines (X[X.chull,])
polygon(X[X.chull,])
?polygon
polygon(X[X.chull,], col = "red")
class(X)
######################################################
######################################################
#
# parallel_plot_functions.r   16 Jan. 2015
#
# Functions for plotting a parallel axis plot.
#
# NOTE: There's still a bug in this code that
# prevents a full matrix of objectives from being
# plotted.  I'm still working on it, so stay tuned!
#
######################################################
normalize.objective <- function(obj.mat) {
apply(obj.mat, 2, function(x) ((x-min(x))/max(x-min(x))))
}
plot.parallel <- function(obj, labels=colnames(obj), col.obj=par("fg"), col.line=par("fg"), col.text=par("fg"), add=F, ...) {
n.objs <- ncol(obj)
if (!add) {
plot.new()
par(xpd=T)
par(mar=c(6,3.5,4,2.5)+0.1)
plot.window(xlim=c(1,n.objs), ylim=c(0,1))
axis(1, at=1:n.objs, labels=F, line=2)
mtext(1, at=1:n.objs, text=labels, col=col.text, line=3, font=2)
arrows(x0=0.9, y0=0.05, y1=0.95, length=0.1)
mtext("Preference", 2)
par(xpd=F)
abline(v=1:n.objs, lty=1, lwd=1.5)
#axis(2,at=seq(0,1,l=5), line=2)
}
cat(nrow(obj))
invisible(sapply(1:nrow(obj), function(this.item) lines(obj[this.item,], col=col.line[this.item], ...)))
lines(1:n.objs, rep(1, n.objs), lty=2, lwd=1.5)
#abline(h=1, lty=2, lwd=1.5)
}
a <- rnorm(5,30,5)
b <- rnorm(5,10,5)
c <- rnorm(5,50,10)
x <- rnorm(5,10,1)
y <- rnorm(5,20,1)
z <- rnorm(5,100,20)
obj.mat <- cbind(a,b,c,x,y,z)
obj.mat.norm <- normalize.objective(obj.mat)
plot.parallel(obj.mat.norm)
############################################
##  file: cost-benefit-slr-adaptation.R
##   R Example of a Sea Level Risk Model
##   - Cost-Benefit analysis of sea wall heightening
##   - Based on:
##	 - van Dantzig D (1956) Economic Decision Problems for flood prevention.
##	 - Econometrica 24:276-287
######################################################
##  authors: Ryan Sriver and Klaus Keller
##  copyright by the authors
##  distributed under the GNU general public license
##  no warranty
##################################################
# version: 3
#   last changes: KK (klaus@psu.edu)
#   Feb 14 2015
##################################################
# how to run:
# - save the file in a directory
# - go to the directory with this file
# - open R
# - type: source('cost-benefit-slr-adaptation.R')
# - open the new pdf files to analyze the results
############################################
# Define parameters as given in van Dantzig
#	- check with Table 4.1 in ref 2
# flood frequency in 1/a at zero height increase
p_zero_p=0.0038
# rate of exponential flood frequency decrease per m height increase
alpha_p=2.6
# costs of flooding in Gulders (include the "factor of two" from the ms
V_p=2e10
# effective discount rate in % per year, this is net of increase in value damages
delta_prime_p=0.02
# costs of heightening per m in gulders.
k_p=4.2e7
# ime horizon to the next revision of the dike (in years), from paper
T=75
# subsidence rate in m/a, from the paper. Equivalent to 0.7 m per century.
subs_rate=0.007
# Example sea_level rise rate in m/a. Equivalent to 0.9 m per century
sea_level_rate=0.009
# time scale
beta_p=alpha_p*1
# parameters for the probabilistic analysis of SLR
# SL_rates in m/a
SL_rate_samples=c(0.003, 0.009, 0.015)
# number of samples
N_SOW=length(SL_rate_samples)
# probabilty of each SOW, assumed so far equally likely
p_SOW=rep(1/N_SOW, N_SOW)
# perform simple risk and economic analysis
# case 1: without uncertainty
# determine optimal X: (eqs. 12 and 14 in paper)
C=(100*p_zero_p*V_p*alpha_p)/((delta_prime_p*100-beta_p)*k_p) *
(1-exp(-(delta_prime_p*100-beta_p)*T/100)) /
(1-exp(-delta_prime_p*100*T/100));
X_analytical=1/alpha_p*log(C)
print(X_analytical)
# costs of heightening per m in gulders.
k_p=4.2e7
# ime horizon to the next revision of the dike (in years), from paper
T=75
# subsidence rate in m/a, from the paper. Equivalent to 0.7 m per century.
subs_rate=0.007
# Example sea_level rise rate in m/a. Equivalent to 0.9 m per century
sea_level_rate=0.009
# time scale
beta_p=alpha_p*1
# parameters for the probabilistic analysis of SLR
# SL_rates in m/a
SL_rate_samples=c(0.003, 0.009, 0.015)
# number of samples
N_SOW=length(SL_rate_samples)
# probabilty of each SOW, assumed so far equally likely
p_SOW=rep(1/N_SOW, N_SOW)
# perform simple risk and economic analysis
# case 1: without uncertainty
# determine optimal X: (eqs. 12 and 14 in paper)
C=(100*p_zero_p*V_p*alpha_p)/((delta_prime_p*100-beta_p)*k_p) *
(1-exp(-(delta_prime_p*100-beta_p)*T/100)) /
(1-exp(-delta_prime_p*100*T/100));
X_analytical=1/alpha_p*log(C)
print(X_analytical)
# case 2:  with uncertainty
# considerd range of Deike heigtening in meters
X=seq(0,10,by=0.05)
# considered time horison, in annual increments
time=seq(0,T,by=1)
# Define variables
p_exceed=array(NA,dim=c(length(X)))
costs=array(NA,dim=c(length(X)))
NPV_expected_losses=array(NA,dim=c(length(X)))
EV_p_exceed_transient=array(NA,dim=c(length(X)))
Total_flood_frequency=array(NA,dim=c(length(X)))
total_costs=array(NA,dim=c(length(X)))
discount_factor=array(NA, dim=c(length(time)))
effective_height=array(NA, dim=c(length(X),length(time)))
p_exceed_transient=array(NA, dim=c(length(X),length(time)))
NPV_costs_flooding=array(NA, dim=c(length(X),length(time)))
subsidence=subs_rate*time
sea_level_rise=sea_level_rate*time
?subset
library(scatterplot3d)
set.seed(1234)
x <- rnorm(30, 10, 3)
y <- rnorm(30, 20, 3)
z <- rnorm(30, 50, 5)
scatterplot3d(x, y, z)
scatterplot3d(x, y, z, pch=15, color="blue", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label")
scatterplot3d(x, y, z, pch=21, color="blue", bg="red", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label",
type="h", angle=58)
my.2dpoints <- my.scatterplot$xyz.convert(10, 20, 50)
text(my.2dpoints$x, my.2dpoints$y, "Mean x-y-z (10,20,50)", pos=4, font=2)
my.scatterplot <- scatterplot3d(x, y, z,
pch=21, color="blue", bg="red", main="This is a title",
xlab="X-label", ylab="Y-label", zlab="Z-label",
type="h", angle=58)
my.scatterplot$points3d(10, 20, 50, col="green", pch=17, cex=2.2)
my.2dpoints <- my.scatterplot$xyz.convert(10, 20, 50)
text(my.2dpoints$x, my.2dpoints$y, "Mean x-y-z (10,20,50)", pos=4, font=2)
rm(list = ls())
graphics.off()
T = 298.15 #K       # Variables can be assigned values using "=" or "<-" (arrow sign)
S = 35 #psu
K_B = exp((-8966.9 - 2890.51 * S^0.5 - 77.942 * S
+ 1.726 * S^1.5 - 0.0993 * S^2)/T + (148.0248
+ 137.194 * S^0.5 + 1.62247 * S)
+ (-24.4344 - 25.085 * S^0.5 - 0.2474 * S) * log(T)  # 'log' is fuction for natural log in R
+ (0.053105 * S^0.5 * T))                            # log10 is function for base 10 log
rm(list = ls())
graphics.off()
T = 298.15 #K       # Variables can be assigned values using "=" or "<-" (arrow sign)
S <- 35 #psu
K_B = exp((-8966.9 - 2890.51 * S^0.5 - 77.942 * S
+ 1.726 * S^1.5 - 0.0993 * S^2)/T + (148.0248
+ 137.194 * S^0.5 + 1.62247 * S)
+ (-24.4344 - 25.085 * S^0.5 - 0.2474 * S) * log(T)  # 'log' is fuction for natural log in R
+ (0.053105 * S^0.5 * T))                            # log10 is function for base 10 log
pK_B = -log10(K_B)                        # (pH at which both species is at 50% equilibrium)
pH = seq(4, 12, by = 0.5)                 # Set vector (sequence) of pH values from 4 - 12 in increments of 0.5
pH_K = 10^-pH                             # set vector of Hydrogen activity, aH+, to achieve pH
ratio = K_B / pH_K                        # Ratio of Borate to Boric Acid (products to reactants)
Borate = (ratio / (ratio + 1)) * 100      # Percent of total solution of Borate ion
Boric_Acid =  100 - Borate                # Percent of total solution of Boric Acid (subtract from 100 to get inverse)
Proportion <- array(NA, dim = c(17,3))    # This creates a blank table with 3 columns and 17 rows, and initializes with
View(Proportion)
View(Proportion)
colnames(Proportion) <- c("pH", "% Boric Acid", "% Borate Ion")   # Sets the names of the columns in the table
View(Proportion)
Proportion[,1] <- pH              # You can isolate individual rows/columns of a matrix/table using the following notation:
Proportion[,2] <- Boric_Acid
Proportion[,3] <- Borate
View(Proportion)                  # Lets you view a variable in a new window
?lty
?col
plot(pH, Boric_Acid,                                # "plot" function begins the plot, with the x and y variable in that order
type = 'l',                                    # "type = 'l'" signifies a line plot
ylim = c(0,100),                               # "ylim" manually sets the min and max y-axis dimensions
lwd = 2,                                       # "lwd" = linewidth. Default is 1, increasing numbers get thicker
pch = 19,                                      # "pch" = point character? Google it (or type '?pch'), there are multiple different point styles to choose
cex = 0.5,                                     # "cex" = character expansion (increases/decreases point size). Default = 1
ylab = "% of total DIC",                       # "ylab" = y-axis label
xlab = "pH")                                   # "xlab" = x-axis label
abline(v = pK_B, col = "gray")                      # "abline" sets straight lines. v=vertical, h=horizontal. You set the value of the y/x intercept
abline(h = 50, col = "gray")
points(pH, Borate, type = 'o',                      # "points" tells the plot to create scatterplot with x=pH, y=Borate. Type "o" is connected points
lty = 2,                                     # "lty" = linetype. Default is 1 (regular line), 2 is dashed
col = "blue",                                # "col" = color. R has multiple colors built in that can be called by name
pch = 19,
cex = 0.5,
lwd = 2)
legend(x="topleft",                                 # "legend" creates a legend. "Topleft" indicates location on plot
inset=c(0.05,0.1),                           # inset the legend into graph
legend=c("Boric Acid", "Borate Ion"),        # sets the name for the elements in the legend
col=c("black","blue"),
lty=c("solid","dashed"),                     # sets linetypes for legend. "solid" corresponds with "Boric Acid"
lwd=c(2,2,2)
)
box(lwd = 2)                                        # "box" just puts a box outline around the plot. Here I wanted to make the outline thicker
test <- seq(1,10, 1)
plot(rnorm(100))
plot(rnorm(100), col = "red")
plot(hist(rnorm(100)))
plot(hist(rnorm(100)))
plot(rnorm(100))
point(rnorm(1000), col = "green")
points(rnorm(1000), col = "green")
?rm.NA
a <- seq(1,20, 1)
b = function(x){
a^2 + 2*a + 2
}
b = function(a){
a^2 + 2*a + 2
}
b
b(a)
plot(a, b(a))
plot(b(a), b(a)/2*b(a))
test <- rnorm(100)
test[100:120] = NA
View(test)
test2 <- sample(test, 1000)
test2 <- sample(test, 1000, replace = T)
View(test2)
?sample
?if
?if
else
help(if)
test2 <- if test =/ NA, sample(test, 1000, replace = T)
?na.omit
test2 <- na.omit(test)
View(test2)
test <- rnorm(100)
test[101:120] = NA
test2 <- na.omit(test)
"test"
print(test)
students <- data.frame(people=c("Lily", "Bo", "Jen", "Omar", "Sara", "Jack", "Ting"), team=c("Red", "Blue", "Green", "Red", "Blue", "Green", "Red"), number=c(1,2,3,5,2,7,1))
View(students)
scores <- data.frame(team=c("Black", "Blue", "Green", "Indigo", "Orange", "Red", "Violet", "White", "Yellow"), score1=c(90,96,93,88,82,84,95,89,79), score2=c(5,5,4,4,5,3,5,5,3))
View(scores)
score1 <- rep(NA, 7)
score2 <- rep(NA, 7)
students <- data.frame(students, score1, score2)
students <- data.frame(people=c("Lily", "Bo", "Jen", "Omar", "Sara", "Jack", "Ting"), team=c("Red", "Blue", "Green", "Red", "Blue", "Green", "Red"), number=c(1,2,3,5,2,7,1))
students <- data.frame(students, score1, score2)
View(students)
students$score1 <- scores$score1[match(students$team, scores$team)]
View(students)
students$score2 <- scores$score2[match(students$team, scores$team)]
students$score2 <- scores$score2[match(students$team, scores$team)]
View(students)
library(ggmap)
library(ggplot2)
library(ggmap)
install.packages("ggplot2")
library(ggmap)
library(zoo)
library(extRemes)
library(fExtremes)
load("~/Documents/Grad/SCRiM/vanDantzig/Model_Versions/Uncertainty_SLR_GEV/SLR_GEV.RData")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev[,1], 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev[,1], 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = "black")
matplot(gev, 1-q, log= 'y', type = 'l',lty = 1, col = rgb(0,0,0,0.05))
dim(EV_p_exceed_transient)
hist(EV_p_exceed_transient[1,])
?rnorm
test <- rnorm(1000, 0, 1)
hist(test)
test
test
?sample
rnorm(100)
hist(rnorm(100))
hist(rnorm(100), breaks = 15)
hist(rnorm(1000), breaks = 15)
hist(rnorm(10000), breaks = 15)
lines(density(rnorm(100)))
2100-2015
85*8
4.25-2.35
print(1, 3, 5)
print(c(1,3,5))
print(1 2 4)
setwd("~/Documents/Grad/SCRiM/san_francisco_GEV")
rm(list = ls())
graphics.off()
# Load required libraries
library(zoo)
library(lubridate)
# Read in tide gauge data
data = read.table("Data/SFB_TG_1914-2014.txt", header = TRUE, sep = '\t')
nrows(data)
nrow(data)
nrow(data)/364/24
data$sl <- data$sl * 100
data[,4] <- NULL
data[,4] <- NULL
data = read.table("Data/SFB_TG_1914-2014.txt", header = TRUE, sep = '\t')
data$sl <- data$sl * 100
View(data)
data$sl <- data$sl * 100
View(data)
rm(list = ls())
graphics.off()
# Load required libraries
library(zoo)
library(lubridate)
# Read in tide gauge data
data = read.table("Data/SFB_TG_1914-2014.txt", header = TRUE, sep = '\t')
data$sl <- data$sl * 100
View(data)
as.year <- function(x) {floor(as.numeric(as.yearmon(x)))}
# Format data - create Datetime column and reference columns for each year in dataset
# date.time column used to create unique index for each entry
data$date2 <- as.Date(as.character(data$date), format="%Y%m%d")
data$date.time <- as.POSIXct(paste(data$date, data$time), format = "%Y%m%d %H:%M", "GMT")
data$year.id <- as.numeric(as.factor(with(data, as.year(paste(date2)))))
length(unique(data$year.id))
sl <- zoo(data$sl, order.by=data$date.time)
length(unique(sl))
length(unique(data$date.time))
year.max <- aggregate(sl, as.year, max)
year.mean <- aggregate(sl, as.year, mean)
annual <- data.frame(index(year.max), coredata(year.mean), coredata(year.max))
annual$year.id <- index(annual[,1])
annual.res <- merge(data[, c("year.id", "sl")], annual[, c("year.id", "coredata.year.mean.")])
annual.res$date <- as.year(data$date2)
annual.res$residual <- annual.res$sl - annual.res$coredata.year.mean.   # Residuals of Annual Means
annual.res$date.time <- data$date.time   # Unique index for residuals
year.res.zoo <- zoo(annual.res$residual, order.by = annual.res$date.time)
year.res.max <- aggregate(year.res.zoo, as.year, max)
MCMC_coredata <- coredata(year.res.max)
save(MCMC_coredata, file = "coredata.RData")
library(extRemes)
library(fExtremes)
library(ismev)
year.res.max.fit <- fevd(coredata(year.res.max))   # extRemes package
year.res.max.fit2 <- gevFit(coredata(year.res.max))   # fExtremes package
year.res.max.fit3 <- gev.fit(coredata(year.res.max), show = FALSE)   # ismev package
print(year.res.max.fit2@fit$par.ests)
year.return <- return.level(year.res.max.fit, return.period = 10000, do.ci = TRUE)
print(year.return)
getwd()
gev.diag(year.res.max.fit3)
year.return <- return.level(year.res.max.fit, return.period = 100, do.ci = TRUE)
print(year.return)
print(year.res.max.fit2@fit$par.ests)
?ismev
test <- fevd(coredata(year.max))   # extRemes package
print(test@fit$par.ests)
test <- gevFit(coredata(year.max))   # extRemes package
print(test@fit$par.ests)
plot(sl)
plot(year.max)
lines(year.res.max, lty = 2, lwd = 4)
lines(index(year.res.max), coredata(year.res.max), lty = 2, lwd = 4)
plot(year.res.max)
mean(sl)
max(sl)
mean(year.res.max)
mean(annual.res$residual)
plot(year.mean)
plot(year.max)
abline(lm(index(year.max)~coredata(year.mean)))
abline(lm(coredata(year.max)~index(year.mean)))
log2(400)
+1
log2(400)+1
log2(100)+1
setwd("~/Documents/Grad/SCRiM/vanDantzig/Model_Versions/Uncertainty_SLR_GEV/Figures/Objective Tradeoffs")
